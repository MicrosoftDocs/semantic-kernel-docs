---
title: LLM AI Embeddings
description: LLM AI Embeddings
author: johnmaeda
ms.topic: memories
ms.author: johnmaeda
ms.date: 02/07/2023
ms.service: semantic-kernel
---

# What are Embeddings?


> [!TIP]
> Memory: Embeddings
> * Embeddings are vectors or arrays of numbers that represent the meaning and the context of tokens processed by the model.
> * They are used to encode and decode input and output texts, and can vary in size and dimension. / Embeddings can help the model understand the relationships between tokens, and generate relevant and coherent texts.
> * They are used for text classification, summarization, translation, and generation, as well as image and code generation.
>
> _ðŸ‘†Notes generated by plugin [`SummarizePlugin.Notegen`](https://github.com/microsoft/semantic-kernel/tree/main/prompt_template_samples/pluginsSummarizePlugin/Notegen)_

_Embeddings_ are the representations or encodings of [tokens](/semantic-kernel/concepts-ai/tokens), such as sentences, paragraphs, or documents, in a high-dimensional vector space, where each dimension corresponds to a learned feature or attribute of the language. Embeddings are the way that the model captures and stores the meaning and the relationships of the language, and the way that the model compares and contrasts different tokens or units of language. Embeddings are the bridge between the discrete and the continuous, and between the symbolic and the numeric, aspects of language for the model.

## What are embeddings to a programmer?

_Embeddings_ are vectors or arrays of numbers that represent the meaning and the context of the tokens that the model processes and generates. Embeddings are derived from the parameters or the weights of the model, and are used to encode and decode the input and output texts. Embeddings can help the model to understand the semantic and syntactic relationships between the tokens, and to generate more relevant and coherent texts. Embeddings can also enable the model to handle multimodal tasks, such as image and code generation, by converting different types of data into a common representation. Embeddings are an essential component of the transformer architecture that GPT-based models use, and they can vary in size and dimension depending on the model and the task.

## How are embeddings used?

_Embeddings_ are used for:

* **Text classification:** Embeddings can help the model to assign labels or categories to texts, based on their meaning and context. For example, embeddings can help the model to classify texts as positive or negative, spam or not spam, news or opinion, etc.
* **Text summarization:** Embeddings can help the model to extract or generate the most important or relevant information from texts, and to create concise and coherent summaries. For example, embeddings can help the model to summarize news articles, product reviews, research papers, etc.
* **Text translation:** Embeddings can help the model to convert texts from one language to another, while preserving the meaning and the structure of the original texts. For example, embeddings can help the model to translate texts between English and Spanish, French and German, Chinese and Japanese, etc.
* **Text generation:** Embeddings can help the model to create new and original texts, based on the input or the prompt that the user provides. For example, embeddings can help the model to generate texts such as stories, poems, jokes, slogans, captions, etc.
* **Image generation:** Embeddings can help the model to create images from texts, or vice versa, by converting different types of data into a common representation. For example, embeddings can help the model to generate images such as logos, faces, animals, landscapes, etc.
* **Code generation:** Embeddings can help the model to create code from texts, or vice versa, by converting different types of data into a common representation. For example, embeddings can help the model to generate code such as HTML, CSS, JavaScript, Python, etc.

## Take the next step

> [!div class="nextstepaction"]
> [Learn about vector databases](/semantic-kernel/concepts-ai/vectordb)

