---
title: Add embedding generation services to Semantic Kernel
description: Learn how to add embedding generation services to your Semantic Kernel project.
zone_pivot_groups: programming-languages
author: westey-m
ms.topic: conceptual
ms.author: westey
ms.date: 11/11/2024
ms.service: semantic-kernel
---

# Text Embedding generation in Semantic Kernel

With text embedding generation, you can use an AI model to generate vectors (aka embeddings). These vectors encode the semantic meaning of the text in such a way that mathematical equations can be used on two vectors to compare the similiarty of the original text.
This is useful for scenarios such as Retrieval Augmented Generation (RAG), where we want to search a database of information for text related to a user query.
Any matching information can then be provided as input to Chat Completion, so that the AI Model has more context when answering the user query.

When choosing an embedding model, you will need to consider the following:

- What is the size of the vectors generated by the model, and is it configurable, as this will affect your vector storage cost.
- What type of elements does the generated vectors contain, e.g. float32, float16, etc, as this will affect your vector storage cost.
- How fast does it generate vectors?
- How much does generation cost?

> [!TIP]
> For more information about storing and searching vectors see [What are Semantic Kernel Vector Store connectors?](../../vector-store-connectors/index.md)
> [!TIP]
> For more information about using RAG with vector stores in Semantic Kernel, see [How to use Vector Stores with Semantic Kernel Text Search](../../text-search/text-search-vector-stores.md) and [What are Semantic Kernel Text Search plugins?](../../text-search/text-search-plugins.md)

::: zone pivot="programming-language-csharp"

## Setting up your local environment

Some of the AI Services can be hosted locally and may require some setup. Below are instructions for those that support this.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

No local setup.

# [OpenAI](#tab/csharp-OpenAI)

No local setup.

# [Mistral](#tab/csharp-Mistral)

No local setup.

# [Google](#tab/csharp-Google)

No local setup.

# [Hugging Face](#tab/csharp-HuggingFace)

No local setup.

# [Ollama](#tab/csharp-Ollama)

To run Ollama locally using docker, use the following command to start a container using the CPU.

```bash
docker run -d -v "c:\temp\ollama:/root/.ollama" -p 11434:11434 --name ollama ollama/ollama
```

To run Ollama locally using docker, use the following command to start a container using GPUs.

```bash
docker run -d --gpus=all -v "c:\temp\ollama:/root/.ollama" -p 11434:11434 --name ollama ollama/ollama
```

After the container has started, launch a Terminal window for the docker container, e.g. if using
docker desktop, choose `Open in Terminal` from actions.

From this terminal download the required models, e.g. here we are downloading the mxbai-embed-large embedding model.

```bash
ollama pull mxbai-embed-large
```

# [ONNX](#tab/csharp-ONNX)

Clone the repository containing the ONNX model you would like to use.

```bash
git clone https://huggingface.co/TaylorAI/bge-micro-v2
```

---

## Installing the necessary packages

Before adding embedding generation to your kernel, you will need to install the necessary packages. Below are the packages you will need to install for each AI service provider.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.AzureOpenAI
```

# [OpenAI](#tab/csharp-OpenAI)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.OpenAI
```

# [Mistral](#tab/csharp-Mistral)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.MistralAI --prerelease
```

# [Google](#tab/csharp-Google)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Google --prerelease
```

# [Hugging Face](#tab/csharp-HuggingFace)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.HuggingFace --prerelease
```

# [Ollama](#tab/csharp-Ollama)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Ollama --prerelease
```

# [ONNX](#tab/csharp-ONNX)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Onnx --prerelease
```

---

## Creating text embedding generation services

Now that you've installed the necessary packages, you can create a text embedding generation service. Below are the several ways you can text create embedding generation services using Semantic Kernel.

### Adding directly to the kernel

To add a text embedding generation service, you can use the following code to add it to the kernel's inner service provider.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

> [!IMPORTANT]
> The Azure OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0010
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddAzureOpenAITextEmbeddingGeneration(
    deploymentName: "NAME_OF_YOUR_DEPLOYMENT", // Name of deployment, e.g. "text-embedding-ada-002".
    endpoint: "YOUR_AZURE_ENDPOINT",           // Name of Azure OpenAI service endpoint, e.g. https://myaiservice.openai.azure.com.
    apiKey: "YOUR_API_KEY",
    modelId: "MODEL_ID",          // Optional name of the underlying model if the deployment name doesn't match the model name, e.g. text-embedding-ada-002.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel.
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used.
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);
Kernel kernel = kernelBuilder.Build();
```

# [OpenAI](#tab/csharp-OpenAI)

> [!IMPORTANT]
> The OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0010
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddOpenAITextEmbeddingGeneration(
    modelId: "MODEL_ID",          // Name of the embedding model, e.g. "text-embedding-ada-002".
    apiKey: "YOUR_API_KEY",
    orgId: "YOUR_ORG_ID",         // Optional organization id.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);
Kernel kernel = kernelBuilder.Build();
```

# [Mistral](#tab/csharp-Mistral)

> [!IMPORTANT]
> The Mistral embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddMistralTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // Name of the embedding model, e.g. "mistral-embed".
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional uri endpoint including the port where MistralAI server is hosted. Default is https://api.mistral.ai.
    serviceId: "SERVICE_ID",            // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Google](#tab/csharp-Google)

> [!IMPORTANT]
> The Google embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Connectors.Google;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddGoogleAIEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",       // Name of the embedding model, e.g. "models/text-embedding-004".
    apiKey: "API_KEY",
    apiVersion: GoogleAIVersion.V1, // Optional
    serviceId: "SERVICE_ID",        // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()    // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Hugging Face](#tab/csharp-HuggingFace)

> [!IMPORTANT]
> The Hugging Face embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddHuggingFaceTextEmbeddingGeneration(
    model: "NAME_OF_MODEL",             // Name of the embedding model.
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional
    serviceId: "SERVICE_ID",            // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Ollama](#tab/csharp-Ollama)

> [!IMPORTANT]
> The Ollama embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddOllamaTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // E.g. "mxbai-embed-large" if mxbai-embed-large was downloaded as described above.
    endpoint: new Uri("YOUR_ENDPOINT"), // E.g. "http://localhost:11434" if Ollama has been started in docker as described above.
    serviceId: "SERVICE_ID"             // Optional; for targeting specific services within Semantic Kernel
);
Kernel kernel = kernelBuilder.Build();
```

# [ONNX](#tab/csharp-ONNX)

> [!IMPORTANT]
> The ONNX embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddBertOnnxTextEmbeddingGeneration(
    onnxModelPath: "PATH_ON_DISK",       // Path to the model on disk e.g. C:\Repos\huggingface\microsoft\TaylorAI\bge-micro-v2\onnx\model.onnx
    vocabPath: "VOCABULARY_PATH_ON_DISK",// Path to the vocabulary file on disk, e.g. C:\Repos\huggingface\TailorAI\bge-micro-v2\vocab.txt
    serviceId: "SERVICE_ID"              // Optional; for targeting specific services within Semantic Kernel
);
Kernel kernel = kernelBuilder.Build();
```

---

### Using dependency injection

If you're using dependency injection, you'll likely want to add your text embedding generation services directly to the service provider. This is helpful if you want to create singletons of your embedding generation services and reuse them in transient kernels.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

> [!IMPORTANT]
> The Azure OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0010
builder.Services.AddAzureOpenAITextEmbeddingGeneration(
    deploymentName: "NAME_OF_YOUR_DEPLOYMENT", // Name of deployment, e.g. "text-embedding-ada-002".
    endpoint: "YOUR_AZURE_ENDPOINT",           // Name of Azure OpenAI service endpoint, e.g. https://myaiservice.openai.azure.com.
    apiKey: "YOUR_API_KEY",
    modelId: "MODEL_ID",          // Optional name of the underlying model if the deployment name doesn't match the model name, e.g. text-embedding-ada-002.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel.
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [OpenAI](#tab/csharp-OpenAI)

> [!IMPORTANT]
> The OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0010
var builder = Host.CreateApplicationBuilder(args);
builder.Services.AddOpenAITextEmbeddingGeneration(
    modelId: "MODEL_ID",          // Name of the embedding model, e.g. "text-embedding-ada-002".
    apiKey: "YOUR_API_KEY",
    orgId: "YOUR_ORG_ID",         // Optional organization id.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [Mistral](#tab/csharp-Mistral)

> [!IMPORTANT]
> The Mistral embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddMistralTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // Name of the embedding model, e.g. "mistral-embed".
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional uri endpoint including the port where MistralAI server is hosted. Default is https://api.mistral.ai.
    serviceId: "SERVICE_ID"             // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [Google](#tab/csharp-Google)

> [!IMPORTANT]
> The Google embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Connectors.Google;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddGoogleAIEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",       // Name of the embedding model, e.g. "models/text-embedding-004".
    apiKey: "API_KEY",
    apiVersion: GoogleAIVersion.V1, // Optional
    serviceId: "SERVICE_ID"         // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [Hugging Face](#tab/csharp-HuggingFace)

> [!IMPORTANT]
> The Hugging Face embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddHuggingFaceTextEmbeddingGeneration(
    model: "NAME_OF_MODEL",             // Name of the embedding model.
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional
    serviceId: "SERVICE_ID",            // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [Ollama](#tab/csharp-Ollama)

> [!IMPORTANT]
> The Ollama embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddOllamaTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // E.g. "mxbai-embed-large" if mxbai-embed-large was downloaded as described above.
    endpoint: new Uri("YOUR_ENDPOINT"), // E.g. "http://localhost:11434" if Ollama has been started in docker as described above.
    serviceId: "SERVICE_ID"             // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [ONNX](#tab/csharp-ONNX)

> [!IMPORTANT]
> The ONNX embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddBertOnnxTextEmbeddingGeneration(
    onnxModelPath: "PATH_ON_DISK",       // Path to the model on disk e.g. C:\Repos\huggingface\microsoft\TaylorAI\bge-micro-v2\onnx\model.onnx
    vocabPath: "VOCABULARY_PATH_ON_DISK",// Path to the vocabulary file on disk, e.g. C:\Repos\huggingface\TailorAI\bge-micro-v2\vocab.txt
    serviceId: "SERVICE_ID"              // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

---

### Creating standalone instances

Lastly, you can create instances of the service directly so that you can either add them to a kernel later or use them directly in your code without ever injecting them into the kernel or in a service provider.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

> [!IMPORTANT]
> The Azure OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel.Connectors.AzureOpenAI;

#pragma warning disable SKEXP0010
AzureOpenAITextEmbeddingGenerationService textEmbeddingGenerationService = new (
    deploymentName: "NAME_OF_YOUR_DEPLOYMENT", // Name of deployment, e.g. "text-embedding-ada-002".
    endpoint: "YOUR_AZURE_ENDPOINT",           // Name of Azure OpenAI service endpoint, e.g. https://myaiservice.openai.azure.com.
    apiKey: "YOUR_API_KEY",
    modelId: "MODEL_ID",          // Optional name of the underlying model if the deployment name doesn't match the model name, e.g. text-embedding-ada-002.
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used.
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);
```

# [OpenAI](#tab/csharp-OpenAI)

> [!IMPORTANT]
> The OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
#pragma warning disable SKEXP0010
using Microsoft.SemanticKernel.Connectors.OpenAI;
OpenAITextEmbeddingGenerationService textEmbeddingGenerationService = new (
    modelId: "MODEL_ID",          // Name of the embedding model, e.g. "text-embedding-ada-002".
    apiKey: "YOUR_API_KEY",
    organization: "YOUR_ORG_ID",  // Optional organization id.
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);
```

# [Mistral](#tab/csharp-Mistral)

> [!IMPORTANT]
> The Mistral embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.MistralAI;

#pragma warning disable SKEXP0070
MistralAITextEmbeddingGenerationService textEmbeddingGenerationService = new (
    modelId: "NAME_OF_MODEL",           // Name of the embedding model, e.g. "mistral-embed".
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional uri endpoint including the port where MistralAI server is hosted. Default is https://api.mistral.ai.
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
```

# [Google](#tab/csharp-Google)

> [!IMPORTANT]
> The Google embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.Google;

#pragma warning disable SKEXP0070
GoogleAITextEmbeddingGenerationService textEmbeddingGenerationService = new (
    modelId: "NAME_OF_MODEL",       // Name of the embedding model, e.g. "models/text-embedding-004".
    apiKey: "API_KEY",
    apiVersion: GoogleAIVersion.V1, // Optional
    httpClient: new HttpClient()    // Optional; for customizing HTTP client
);
```

# [Hugging Face](#tab/csharp-HuggingFace)

> [!IMPORTANT]
> The Hugging Face embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.HuggingFace;

#pragma warning disable SKEXP0070
HuggingFaceTextEmbeddingGenerationService textEmbeddingGenerationService = new (
    model: "NAME_OF_MODEL",             // Name of the embedding model.
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
```

# [Ollama](#tab/csharp-Ollama)

> [!IMPORTANT]
> The Ollama embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Embeddings;
using OllamaSharp;

#pragma warning disable SKEXP0070
using var ollamaClient = new OllamaApiClient(
    uriString: "YOUR_ENDPOINT"    // E.g. "http://localhost:11434" if Ollama has been started in docker as described above.
    defaultModel: "NAME_OF_MODEL" // E.g. "mxbai-embed-large" if mxbai-embed-large was downloaded as described above.
);

ITextEmbeddingGenerationService textEmbeddingGenerationService = ollamaClient.AsTextEmbeddingGenerationService();
```

# [ONNX](#tab/csharp-ONNX)

> [!IMPORTANT]
> The ONNX embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.Onnx;

#pragma warning disable SKEXP0070
BertOnnxTextEmbeddingGenerationService textEmbeddingGenerationService = await BertOnnxTextEmbeddingGenerationService.CreateAsync(
    onnxModelPath: "PATH_ON_DISK",       // Path to the model on disk e.g. C:\Repos\huggingface\microsoft\TaylorAI\bge-micro-v2\onnx\model.onnx
    vocabPath: "VOCABULARY_PATH_ON_DISK" // Path to the vocabulary file on disk, e.g. C:\Repos\huggingface\TailorAI\bge-micro-v2\vocab.txt
);
```

---

## Using text embedding generation services

All text embedding generation services implement the `ITextEmbeddingGenerationService` which has a single method `GenerateEmbeddingsAsync`
that can generate `ReadOnlyMemory<float>` vectors from provided `string` values.
An extension method `GenerateEmbeddingAsync` is also available for single value versions of the same action.

Here is an example of how to invoke the service with multiple values.

```csharp
IList<ReadOnlyMemory<float>> embeddings =
    await textEmbeddingGenerationService.GenerateEmbeddingsAsync(
    [
        "sample text 1",
        "sample text 2"
    ]);
```

Here is an example of how to invoke the service with a single value.

```csharp
using Microsoft.SemanticKernel.Embeddings;

ReadOnlyMemory<float> embedding =
    await textEmbeddingGenerationService.GenerateEmbeddingAsync("sample text");
```

::: zone-end

::: zone pivot="programming-language-python"

## Coming soon

More information coming soon.

::: zone-end

::: zone pivot="programming-language-java"

## Coming soon

More information coming soon.

::: zone-end
