---
title: Add embedding generation services to Semantic Kernel
description: Learn how to add embedding generation services to your Semantic Kernel project.
zone_pivot_groups: programming-languages
author: westey-m
ms.topic: conceptual
ms.author: westey
ms.date: 11/11/2024
ms.service: semantic-kernel
---

# Embedding generation

With embedding generation, you can use an AI model to generate vectors (aka embeddings). These vectors encode the semantic meaning of the text in such a way that mathematical equations can be used on two vectors to compare the similiarty of the original text.
This is useful for scenarios such as Retrieval Augmented Generation (RAG), where we want to search a database of information, for text related to a user query.
This information can then be provided as input to Chat Completion, so that the AI Model has more context when answering the user query.

When choosing an embedding model, you will need to consider the following:

- What is the size of the vectors generated by the model, and is it configurable, as this will affect your vector storage cost.
- What type of elements does the generated vectors contain, e.g. float32, float16, etc, as this will affect your vector storage cost.
- How fast does it generate vectors?
- How much does generation cost?

> [!TIP]
> For more information about storing and searching vectors see [What are Semantic Kernel Vector Store connectors?](../../vector-store-connectors/index.md)
> [!TIP]
> For more information about using RAG with vector stores in Semantic Kernel, see [How to use Vector Stores with Semantic Kernel Text Search](../../text-search/text-search-vector-stores.md) and [What are Semantic Kernel Text Search plugins?](../../text-search/text-search-plugins.md)

::: zone pivot="programming-language-csharp"

## Setting up your local environment

Some of the AI Services can be hosted locally and may require some setup. Below are instructions for those that support this.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

No local setup.

# [OpenAI](#tab/csharp-OpenAI)

No local setup.

# [Mistral](#tab/csharp-Mistral)

No local setup.

# [Google](#tab/csharp-Google)

No local setup.

# [Hugging Face](#tab/csharp-HuggingFace)

No local setup.

# [Ollama](#tab/csharp-Ollama)

To run Ollama locally using docker, use the following command to start a container using the CPU.

```bash
docker run -d -v "c:\temp\ollama:/root/.ollama" -p 11434:11434 --name ollama ollama/ollama
```

To run Ollama locally using docker, use the following command to start a container using GPUs.

```bash
docker run -d --gpus=all -v "c:\temp\ollama:/root/.ollama" -p 11434:11434 --name ollama ollama/ollama
```

After the container has started, launch a Terminal window for the docker container, e.g. if using
docker desktop, choose `Open in Terminal` from actions.

From this terminal download the required models, e.g. here we are downloading the mxbai-embed-large embedding model.

```bash
ollama pull mxbai-embed-large
```

# [ONNX](#tab/csharp-ONNX)

Clone the repository containing the ONNX model you would like to use.

```bash
git clone https://huggingface.co/TaylorAI/bge-micro-v2
```

---

## Installing the necessary packages

Before adding embedding generation to your kernel, you will need to install the necessary packages. Below are the packages you will need to install for each AI service provider.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.AzureOpenAI
```

# [OpenAI](#tab/csharp-OpenAI)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.OpenAI
```

# [Mistral](#tab/csharp-Mistral)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.MistralAI
```

# [Google](#tab/csharp-Google)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Google
```

# [Hugging Face](#tab/csharp-HuggingFace)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.HuggingFace
```

# [Ollama](#tab/csharp-Ollama)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Ollama
```

# [ONNX](#tab/csharp-ONNX)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Onnx
```

---

## Creating embedding generation services

Now that you've installed the necessary packages, you can create an embedding generation service. Below are the several ways you can create embedding generation services using Semantic Kernel.

### Adding directly to the kernel

To add an embedding generation service, you can use the following code to add it to the kernel's inner service provider.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

```csharp
using Microsoft.SemanticKernel;

IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddAzureOpenAITextEmbeddingGeneration(
    deploymentName: "NAME_OF_YOUR_DEPLOYMENT", // Name of deployment, e.g. "text-embedding-ada-002".
    endpoint: "YOUR_AZURE_ENDPOINT",           // Name of Azure Open AI service endpoint, e.g. https://myaiservice.openai.azure.com.
    apiKey: "YOUR_API_KEY",
    modelId: "MODEL_ID",          // Optional name of the underlying model if the deployment name doesn't match the model name, e.g. text-embedding-ada-002.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel.
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used.
    dimensions: 1536
);
Kernel kernel = kernelBuilder.Build();
```

# [OpenAI](#tab/csharp-OpenAI)

```csharp
using Microsoft.SemanticKernel;

IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddOpenAITextEmbeddingGeneration(
    modelId: "MODEL_ID",          // Name of the embedding model, e.g. "text-embedding-ada-002".
    apiKey: "YOUR_API_KEY",
    orgId: "YOUR_ORG_ID",         // Optional organization id.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()  // Optional; if not provided, the HttpClient from the kernel will be used
);
Kernel kernel = kernelBuilder.Build();
```

# [Mistral](#tab/csharp-Mistral)

> [!IMPORTANT]
> The Mistral embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddMistralChatCompletion(
    modelId: "NAME_OF_MODEL",           // Name of the embedding model, e.g. "mistral-embed".
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional uri endpoint including the port where MistralAI server is hosted. Default is https://api.mistral.ai.
    serviceId: "SERVICE_ID",            // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Google](#tab/csharp-Google)

> [!IMPORTANT]
> The Google embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Connectors.Google;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddGoogleAIEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",       // Name of the embedding model, e.g. "models/text-embedding-004".
    apiKey: "API_KEY",
    apiVersion: GoogleAIVersion.V1, // Optional
    serviceId: "SERVICE_ID",        // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()    // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Hugging Face](#tab/csharp-HuggingFace)

> [!IMPORTANT]
> The Hugging Face embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddHuggingFaceChatCompletion(
    model: "NAME_OF_MODEL",             // Name of the embedding model.
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional
    serviceId: "SERVICE_ID",            // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Ollama](#tab/csharp-Ollama)

> [!IMPORTANT]
> The Ollama embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddOllamaTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // E.g. "mxbai-embed-large" if mxbai-embed-large was downloaded as described above.
    endpoint: new Uri("YOUR_ENDPOINT"), // E.g. "http://localhost:11434" if Ollama has been started in docker as described above.
    serviceId: "SERVICE_ID"             // Optional; for targeting specific services within Semantic Kernel
);
Kernel kernel = kernelBuilder.Build();
```

# [ONNX](#tab/csharp-ONNX)

> [!IMPORTANT]
> The ONNX embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddBertOnnxTextEmbeddingGeneration(
    onnxModelPath: "PATH_ON_DISK",       // Path to the model on disk e.g. C:\Repos\huggingface\microsoft\TaylorAI\bge-micro-v2\onnx\model.onnx
    vocabPath: "VOCABULARY_PATH_ON_DISK",// Path to the vocabulary file on disk, e.g. C:\Repos\huggingface\TailorAI\bge-micro-v2\vocab.txt
    serviceId: "SERVICE_ID"              // Optional; for targeting specific services within Semantic Kernel
);
Kernel kernel = kernelBuilder.Build();
```

::: zone-end

::: zone pivot="programming-language-python"

## Coming soon

More information coming soon.

::: zone-end

::: zone pivot="programming-language-java"

## Coming soon

More information coming soon.

::: zone-end
