---
title: Add embedding generation services to Semantic Kernel
description: Learn how to add embedding generation services to your Semantic Kernel project.
zone_pivot_groups: programming-languages
author: westey-m
ms.topic: conceptual
ms.author: westey
ms.date: 11/11/2024
ms.service: semantic-kernel
---

# Text Embedding generation in Semantic Kernel

With text embedding generation, you can use an AI model to generate vectors (aka embeddings). These vectors encode the semantic meaning of the text in such a way that mathematical equations can be used on two vectors to compare the similarity of the original text.
This is useful for scenarios such as Retrieval Augmented Generation (RAG), where we want to search a database of information for text related to a user query.
Any matching information can then be provided as input to Chat Completion, so that the AI Model has more context when answering the user query.

When choosing an embedding model, you will need to consider the following:

- What is the size of the vectors generated by the model, and is it configurable, as this will affect your vector storage cost.
- What type of elements does the generated vectors contain, e.g. float32, float16, etc, as this will affect your vector storage cost.
- How fast does it generate vectors?
- How much does generation cost?

> [!TIP]
> For more information about storing and searching vectors see [What are Semantic Kernel Vector Store connectors?](../../vector-store-connectors/index.md)
> [!TIP]
> For more information about using RAG with vector stores in Semantic Kernel, see [How to use Vector Stores with Semantic Kernel Text Search](../../text-search/text-search-vector-stores.md) and [What are Semantic Kernel Text Search plugins?](../../text-search/text-search-plugins.md)

::: zone pivot="programming-language-csharp"

## Setting up your local environment

Some of the AI Services can be hosted locally and may require some setup. Below are instructions for those that support this.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

No local setup.

# [OpenAI](#tab/csharp-OpenAI)

No local setup.

# [Mistral](#tab/csharp-Mistral)

No local setup.

# [Google](#tab/csharp-Google)

No local setup.

# [Hugging Face](#tab/csharp-HuggingFace)

No local setup.

# [Ollama](#tab/csharp-Ollama)

To run Ollama locally using docker, use the following command to start a container using the CPU.

```bash
docker run -d -v "c:\temp\ollama:/root/.ollama" -p 11434:11434 --name ollama ollama/ollama
```

To run Ollama locally using docker, use the following command to start a container using GPUs.

```bash
docker run -d --gpus=all -v "c:\temp\ollama:/root/.ollama" -p 11434:11434 --name ollama ollama/ollama
```

After the container has started, launch a Terminal window for the docker container, e.g. if using
docker desktop, choose `Open in Terminal` from actions.

From this terminal download the required models, e.g. here we are downloading the mxbai-embed-large embedding model.

```bash
ollama pull mxbai-embed-large
```

# [ONNX](#tab/csharp-ONNX)

Clone the repository containing the ONNX model you would like to use.

```bash
git clone https://huggingface.co/TaylorAI/bge-micro-v2
```

# [VoyageAI](#tab/csharp-VoyageAI)

No local setup.

---

## Installing the necessary packages

Before adding embedding generation to your kernel, you will need to install the necessary packages. Below are the packages you will need to install for each AI service provider.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.AzureOpenAI
```

# [OpenAI](#tab/csharp-OpenAI)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.OpenAI
```

# [Mistral](#tab/csharp-Mistral)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.MistralAI --prerelease
```

# [Google](#tab/csharp-Google)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Google --prerelease
```

# [Hugging Face](#tab/csharp-HuggingFace)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.HuggingFace --prerelease
```

# [Ollama](#tab/csharp-Ollama)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Ollama --prerelease
```

# [ONNX](#tab/csharp-ONNX)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.Onnx --prerelease
```

# [VoyageAI](#tab/csharp-VoyageAI)

```bash
dotnet add package Microsoft.SemanticKernel.Connectors.VoyageAI --prerelease
```

---

## Creating text embedding generation services

Now that you've installed the necessary packages, you can create a text embedding generation service. Below are the several ways you can text create embedding generation services using Semantic Kernel.

### Adding directly to the kernel

To add a text embedding generation service, you can use the following code to add it to the kernel's inner service provider.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

> [!IMPORTANT]
> The Azure OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0010
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddAzureOpenAITextEmbeddingGeneration(
    deploymentName: "NAME_OF_YOUR_DEPLOYMENT", // Name of deployment, e.g. "text-embedding-ada-002".
    endpoint: "YOUR_AZURE_ENDPOINT",           // Name of Azure OpenAI service endpoint, e.g. https://myaiservice.openai.azure.com.
    apiKey: "YOUR_API_KEY",
    modelId: "MODEL_ID",          // Optional name of the underlying model if the deployment name doesn't match the model name, e.g. text-embedding-ada-002.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel.
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used.
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);
Kernel kernel = kernelBuilder.Build();
```

# [OpenAI](#tab/csharp-OpenAI)

> [!IMPORTANT]
> The OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0010
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddOpenAITextEmbeddingGeneration(
    modelId: "MODEL_ID",          // Name of the embedding model, e.g. "text-embedding-ada-002".
    apiKey: "YOUR_API_KEY",
    orgId: "YOUR_ORG_ID",         // Optional organization id.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);
Kernel kernel = kernelBuilder.Build();
```

# [Mistral](#tab/csharp-Mistral)

> [!IMPORTANT]
> The Mistral embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddMistralTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // Name of the embedding model, e.g. "mistral-embed".
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional uri endpoint including the port where MistralAI server is hosted. Default is https://api.mistral.ai.
    serviceId: "SERVICE_ID",            // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Google](#tab/csharp-Google)

> [!IMPORTANT]
> The Google embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Connectors.Google;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddGoogleAIEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",       // Name of the embedding model, e.g. "models/text-embedding-004".
    apiKey: "API_KEY",
    apiVersion: GoogleAIVersion.V1, // Optional
    serviceId: "SERVICE_ID",        // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()    // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Hugging Face](#tab/csharp-HuggingFace)

> [!IMPORTANT]
> The Hugging Face embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddHuggingFaceTextEmbeddingGeneration(
    model: "NAME_OF_MODEL",             // Name of the embedding model.
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional
    serviceId: "SERVICE_ID",            // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
Kernel kernel = kernelBuilder.Build();
```

# [Ollama](#tab/csharp-Ollama)

> [!IMPORTANT]
> The Ollama embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddOllamaTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // E.g. "mxbai-embed-large" if mxbai-embed-large was downloaded as described above.
    endpoint: new Uri("YOUR_ENDPOINT"), // E.g. "http://localhost:11434" if Ollama has been started in docker as described above.
    serviceId: "SERVICE_ID"             // Optional; for targeting specific services within Semantic Kernel
);
Kernel kernel = kernelBuilder.Build();
```

# [ONNX](#tab/csharp-ONNX)

> [!IMPORTANT]
> The ONNX embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddBertOnnxTextEmbeddingGeneration(
    onnxModelPath: "PATH_ON_DISK",       // Path to the model on disk e.g. C:\Repos\huggingface\microsoft\TaylorAI\bge-micro-v2\onnx\model.onnx
    vocabPath: "VOCABULARY_PATH_ON_DISK",// Path to the vocabulary file on disk, e.g. C:\Repos\huggingface\TailorAI\bge-micro-v2\vocab.txt
    serviceId: "SERVICE_ID"              // Optional; for targeting specific services within Semantic Kernel
);
Kernel kernel = kernelBuilder.Build();
```

# [VoyageAI](#tab/csharp-VoyageAI)

> [!IMPORTANT]
> The VoyageAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0070
IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
kernelBuilder.AddVoyageAITextEmbeddingGeneration(
    modelId: "voyage-3-large",           // Name of the VoyageAI model, e.g. "voyage-3-large", "voyage-code-3", "voyage-finance-2"
    apiKey: "YOUR_API_KEY",
    endpoint: "https://api.voyageai.com/v1", // Optional; VoyageAI API endpoint
    serviceId: "YOUR_SERVICE_ID",        // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()         // Optional; if not provided, the HttpClient from the kernel will be used
);
Kernel kernel = kernelBuilder.Build();
```

---

### Using dependency injection

If you're using dependency injection, you'll likely want to add your text embedding generation services directly to the service provider. This is helpful if you want to create singletons of your embedding generation services and reuse them in transient kernels.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

> [!IMPORTANT]
> The Azure OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0010
builder.Services.AddAzureOpenAITextEmbeddingGeneration(
    deploymentName: "NAME_OF_YOUR_DEPLOYMENT", // Name of deployment, e.g. "text-embedding-ada-002".
    endpoint: "YOUR_AZURE_ENDPOINT",           // Name of Azure OpenAI service endpoint, e.g. https://myaiservice.openai.azure.com.
    apiKey: "YOUR_API_KEY",
    modelId: "MODEL_ID",          // Optional name of the underlying model if the deployment name doesn't match the model name, e.g. text-embedding-ada-002.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel.
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [OpenAI](#tab/csharp-OpenAI)

> [!IMPORTANT]
> The OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel;

#pragma warning disable SKEXP0010
var builder = Host.CreateApplicationBuilder(args);
builder.Services.AddOpenAITextEmbeddingGeneration(
    modelId: "MODEL_ID",          // Name of the embedding model, e.g. "text-embedding-ada-002".
    apiKey: "YOUR_API_KEY",
    orgId: "YOUR_ORG_ID",         // Optional organization id.
    serviceId: "YOUR_SERVICE_ID", // Optional; for targeting specific services within Semantic Kernel
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [Mistral](#tab/csharp-Mistral)

> [!IMPORTANT]
> The Mistral embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddMistralTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // Name of the embedding model, e.g. "mistral-embed".
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional uri endpoint including the port where MistralAI server is hosted. Default is https://api.mistral.ai.
    serviceId: "SERVICE_ID"             // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [Google](#tab/csharp-Google)

> [!IMPORTANT]
> The Google embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Connectors.Google;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddGoogleAIEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",       // Name of the embedding model, e.g. "models/text-embedding-004".
    apiKey: "API_KEY",
    apiVersion: GoogleAIVersion.V1, // Optional
    serviceId: "SERVICE_ID"         // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [Hugging Face](#tab/csharp-HuggingFace)

> [!IMPORTANT]
> The Hugging Face embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddHuggingFaceTextEmbeddingGeneration(
    model: "NAME_OF_MODEL",             // Name of the embedding model.
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional
    serviceId: "SERVICE_ID",            // Optional; for targeting specific services within Semantic Kernel
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [Ollama](#tab/csharp-Ollama)

> [!IMPORTANT]
> The Ollama embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddOllamaTextEmbeddingGeneration(
    modelId: "NAME_OF_MODEL",           // E.g. "mxbai-embed-large" if mxbai-embed-large was downloaded as described above.
    endpoint: new Uri("YOUR_ENDPOINT"), // E.g. "http://localhost:11434" if Ollama has been started in docker as described above.
    serviceId: "SERVICE_ID"             // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [ONNX](#tab/csharp-ONNX)

> [!IMPORTANT]
> The ONNX embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddBertOnnxTextEmbeddingGeneration(
    onnxModelPath: "PATH_ON_DISK",       // Path to the model on disk e.g. C:\Repos\huggingface\microsoft\TaylorAI\bge-micro-v2\onnx\model.onnx
    vocabPath: "VOCABULARY_PATH_ON_DISK",// Path to the vocabulary file on disk, e.g. C:\Repos\huggingface\TailorAI\bge-micro-v2\vocab.txt
    serviceId: "SERVICE_ID"              // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

# [VoyageAI](#tab/csharp-VoyageAI)

> [!IMPORTANT]
> The VoyageAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel;

var builder = Host.CreateApplicationBuilder(args);

#pragma warning disable SKEXP0070
builder.Services.AddVoyageAITextEmbeddingGeneration(
    modelId: "voyage-3-large",                  // Name of the VoyageAI model
    apiKey: "YOUR_API_KEY",
    endpoint: "https://api.voyageai.com/v1",    // Optional; VoyageAI API endpoint
    serviceId: "SERVICE_ID"                     // Optional; for targeting specific services within Semantic Kernel
);

builder.Services.AddTransient((serviceProvider)=> {
    return new Kernel(serviceProvider);
});
```

---

### Creating standalone instances

Lastly, you can create instances of the service directly so that you can either add them to a kernel later or use them directly in your code without ever injecting them into the kernel or in a service provider.

# [Azure OpenAI](#tab/csharp-AzureOpenAI)

> [!IMPORTANT]
> The Azure OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
using Microsoft.SemanticKernel.Connectors.AzureOpenAI;

#pragma warning disable SKEXP0010
AzureOpenAITextEmbeddingGenerationService textEmbeddingGenerationService = new (
    deploymentName: "NAME_OF_YOUR_DEPLOYMENT", // Name of deployment, e.g. "text-embedding-ada-002".
    endpoint: "YOUR_AZURE_ENDPOINT",           // Name of Azure OpenAI service endpoint, e.g. https://myaiservice.openai.azure.com.
    apiKey: "YOUR_API_KEY",
    modelId: "MODEL_ID",          // Optional name of the underlying model if the deployment name doesn't match the model name, e.g. text-embedding-ada-002.
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used.
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);
```

# [OpenAI](#tab/csharp-OpenAI)

> [!IMPORTANT]
> The OpenAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0010`.

```csharp
#pragma warning disable SKEXP0010
using Microsoft.SemanticKernel.Connectors.OpenAI;
OpenAITextEmbeddingGenerationService textEmbeddingGenerationService = new (
    modelId: "MODEL_ID",          // Name of the embedding model, e.g. "text-embedding-ada-002".
    apiKey: "YOUR_API_KEY",
    organization: "YOUR_ORG_ID",  // Optional organization id.
    httpClient: new HttpClient(), // Optional; if not provided, the HttpClient from the kernel will be used
    dimensions: 1536              // Optional number of dimensions to generate embeddings with.
);
```

# [Mistral](#tab/csharp-Mistral)

> [!IMPORTANT]
> The Mistral embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.MistralAI;

#pragma warning disable SKEXP0070
MistralAITextEmbeddingGenerationService textEmbeddingGenerationService = new (
    modelId: "NAME_OF_MODEL",           // Name of the embedding model, e.g. "mistral-embed".
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional uri endpoint including the port where MistralAI server is hosted. Default is https://api.mistral.ai.
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
```

# [Google](#tab/csharp-Google)

> [!IMPORTANT]
> The Google embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.Google;

#pragma warning disable SKEXP0070
GoogleAITextEmbeddingGenerationService textEmbeddingGenerationService = new (
    modelId: "NAME_OF_MODEL",       // Name of the embedding model, e.g. "models/text-embedding-004".
    apiKey: "API_KEY",
    apiVersion: GoogleAIVersion.V1, // Optional
    httpClient: new HttpClient()    // Optional; for customizing HTTP client
);
```

# [Hugging Face](#tab/csharp-HuggingFace)

> [!IMPORTANT]
> The Hugging Face embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.HuggingFace;

#pragma warning disable SKEXP0070
HuggingFaceTextEmbeddingGenerationService textEmbeddingGenerationService = new (
    model: "NAME_OF_MODEL",             // Name of the embedding model.
    apiKey: "API_KEY",
    endpoint: new Uri("YOUR_ENDPOINT"), // Optional
    httpClient: new HttpClient()        // Optional; for customizing HTTP client
);
```

# [Ollama](#tab/csharp-Ollama)

> [!IMPORTANT]
> The Ollama embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Embeddings;
using OllamaSharp;

#pragma warning disable SKEXP0070
using var ollamaClient = new OllamaApiClient(
    uriString: "YOUR_ENDPOINT"    // E.g. "http://localhost:11434" if Ollama has been started in docker as described above.
    defaultModel: "NAME_OF_MODEL" // E.g. "mxbai-embed-large" if mxbai-embed-large was downloaded as described above.
);

ITextEmbeddingGenerationService textEmbeddingGenerationService = ollamaClient.AsTextEmbeddingGenerationService();
```

# [ONNX](#tab/csharp-ONNX)

> [!IMPORTANT]
> The ONNX embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.Onnx;

#pragma warning disable SKEXP0070
BertOnnxTextEmbeddingGenerationService textEmbeddingGenerationService = await BertOnnxTextEmbeddingGenerationService.CreateAsync(
    onnxModelPath: "PATH_ON_DISK",       // Path to the model on disk e.g. C:\Repos\huggingface\microsoft\TaylorAI\bge-micro-v2\onnx\model.onnx
    vocabPath: "VOCABULARY_PATH_ON_DISK" // Path to the vocabulary file on disk, e.g. C:\Repos\huggingface\TailorAI\bge-micro-v2\vocab.txt
);
```

# [VoyageAI](#tab/csharp-VoyageAI)

> [!IMPORTANT]
> The VoyageAI embedding generation connector is currently experimental. To use it, you will need to add `#pragma warning disable SKEXP0070`.

```csharp
using Microsoft.SemanticKernel.Connectors.VoyageAI;

#pragma warning disable SKEXP0070
VoyageAITextEmbeddingGenerationService textEmbeddingGenerationService = new(
    modelId: "voyage-3-large",                  // Name of the VoyageAI model
    apiKey: "YOUR_API_KEY",
    endpoint: "https://api.voyageai.com/v1",    // Optional; VoyageAI API endpoint
    httpClient: new HttpClient()                // Optional; for customizing HTTP client
);
```

---

## Using text embedding generation services

All text embedding generation services implement the `ITextEmbeddingGenerationService` which has a single method `GenerateEmbeddingsAsync`
that can generate `ReadOnlyMemory<float>` vectors from provided `string` values.
An extension method `GenerateEmbeddingAsync` is also available for single value versions of the same action.

Here is an example of how to invoke the service with multiple values.

```csharp
IList<ReadOnlyMemory<float>> embeddings =
    await textEmbeddingGenerationService.GenerateEmbeddingsAsync(
    [
        "sample text 1",
        "sample text 2"
    ]);
```

Here is an example of how to invoke the service with a single value.

```csharp
using Microsoft.SemanticKernel.Embeddings;

ReadOnlyMemory<float> embedding =
    await textEmbeddingGenerationService.GenerateEmbeddingAsync("sample text");
```

::: zone-end

::: zone pivot="programming-language-python"

## Installing the necessary packages

Before adding embedding generation to your kernel, you will need to install the necessary packages. Below are the packages you will need to install for each AI service provider.

# [Azure OpenAI](#tab/python-AzureOpenAI)

```bash
pip install semantic-kernel
```

# [OpenAI](#tab/python-OpenAI)

```bash
pip install semantic-kernel
```

# [VoyageAI](#tab/python-VoyageAI)

```bash
pip install semantic-kernel voyageai
```

---

## Creating text embedding generation services

Now that you've installed the necessary packages, you can create a text embedding generation service.

# [Azure OpenAI](#tab/python-AzureOpenAI)

```python
from semantic_kernel.connectors.ai.open_ai import AzureTextEmbedding

text_embedding_service = AzureTextEmbedding(
    deployment_name="YOUR_DEPLOYMENT_NAME",
    endpoint="YOUR_AZURE_ENDPOINT",
    api_key="YOUR_API_KEY",
)
```

# [OpenAI](#tab/python-OpenAI)

```python
from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding

text_embedding_service = OpenAITextEmbedding(
    ai_model_id="text-embedding-ada-002",
    api_key="YOUR_API_KEY",
)
```

# [VoyageAI](#tab/python-VoyageAI)

```python
from semantic_kernel.connectors.ai.voyage_ai import VoyageAITextEmbedding
import os

text_embedding_service = VoyageAITextEmbedding(
    ai_model_id="voyage-3-large",  # Options: voyage-3-large, voyage-code-3, voyage-finance-2, voyage-law-2
    api_key=os.getenv("VOYAGE_API_KEY"),
)
```

---

## Using text embedding generation services

All text embedding generation services implement the embedding generation interface which provides methods to generate embeddings.

### Generating embeddings

```python
import asyncio

async def generate_embeddings():
    texts = ["First text", "Second text", "Third text"]

    embeddings = await text_embedding_service.generate_embeddings(texts)

    for i, embedding in enumerate(embeddings):
        print(f"Embedding {i} dimension: {len(embedding)}")

asyncio.run(generate_embeddings())
```

## VoyageAI Specialized Services

VoyageAI provides additional specialized embedding services for advanced use cases.

### Contextualized Embeddings

For RAG applications where you need document-aware chunk embeddings:

```python
from semantic_kernel.connectors.ai.voyage_ai import VoyageAIContextualizedEmbedding
import os

# Create service
embedding_service = VoyageAIContextualizedEmbedding(
    ai_model_id="voyage-context-3",
    api_key=os.getenv("VOYAGE_API_KEY"),
)

# Each inner list represents chunks from a document
inputs = [
    ["Chapter 1: Introduction", "This chapter covers...", "Key concepts include..."],
    ["Chapter 2: Methods", "We employed the following..."]
]

embeddings = await embedding_service.generate_contextualized_embeddings(inputs)
print(f"Generated {len(embeddings)} contextualized embeddings")
```

### Multimodal Embeddings

For text and image embeddings:

```python
from semantic_kernel.connectors.ai.voyage_ai import VoyageAIMultimodalEmbedding
from PIL import Image
import os

# Create service
embedding_service = VoyageAIMultimodalEmbedding(
    ai_model_id="voyage-multimodal-3",  # Options: voyage-multimodal-3, voyage-multimodal-3.5
    api_key=os.getenv("VOYAGE_API_KEY"),
)

# Text only
texts = ["Text description 1", "Text description 2"]
embeddings = await embedding_service.generate_embeddings(texts)
print(f"Generated {len(embeddings)} multimodal embeddings")

# Mixed text and images
image = Image.open("photo.jpg")
inputs = [
    ["Chapter title", image, "Text after image"]
]
embeddings = await embedding_service.generate_multimodal_embeddings(inputs)
```

### Document Reranking

VoyageAI also provides document reranking for improved search results:

```python
from semantic_kernel.connectors.ai.voyage_ai import VoyageAIReranker
import os

# Create reranker service
reranker_service = VoyageAIReranker(
    ai_model_id="rerank-2.5",
    api_key=os.getenv("VOYAGE_API_KEY"),
)

# Rerank documents
query = "What is Semantic Kernel?"
documents = ["Doc 1 text", "Doc 2 text", "Doc 3 text"]

results = await reranker_service.rerank(query, documents)

# Results are sorted by relevance
for result in results:
    print(f"Index: {result.index}, Score: {result.relevance_score}")
```

### Execution Settings

You can customize the embedding generation with execution settings:

```python
from semantic_kernel.connectors.ai.voyage_ai import VoyageAIEmbeddingPromptExecutionSettings

settings = VoyageAIEmbeddingPromptExecutionSettings(
    input_type="query",  # or "document"
    truncation=True,
    output_dimension=1024,
    output_dtype="float"  # or "int8", "uint8", "binary", "ubinary"
)

embeddings = await text_embedding_service.generate_embeddings(texts, settings=settings)
```

::: zone-end

::: zone pivot="programming-language-java"

## Installing the necessary packages

Before adding embedding generation to your kernel, you will need to install the necessary packages. Below are the packages you will need to install for each AI service provider.

# [Azure OpenAI](#tab/java-AzureOpenAI)

Add the dependency to your `pom.xml`:

```xml
<dependency>
    <groupId>com.microsoft.semantic-kernel</groupId>
    <artifactId>semantickernel-aiservices-openai</artifactId>
    <version>${semantickernel.version}</version>
</dependency>
```

# [OpenAI](#tab/java-OpenAI)

Add the dependency to your `pom.xml`:

```xml
<dependency>
    <groupId>com.microsoft.semantic-kernel</groupId>
    <artifactId>semantickernel-aiservices-openai</artifactId>
    <version>${semantickernel.version}</version>
</dependency>
```

# [VoyageAI](#tab/java-VoyageAI)

Add the dependency to your `pom.xml`:

```xml
<dependency>
    <groupId>com.microsoft.semantic-kernel</groupId>
    <artifactId>semantickernel-aiservices-voyageai</artifactId>
    <version>${semantickernel.version}</version>
</dependency>
```

---

## Creating text embedding generation services

Now that you've installed the necessary packages, you can create a text embedding generation service.

# [Azure OpenAI](#tab/java-AzureOpenAI)

```java
import com.microsoft.semantickernel.aiservices.openai.textembeddings.OpenAITextEmbeddingGenerationService;

OpenAITextEmbeddingGenerationService textEmbeddingService = OpenAITextEmbeddingGenerationService.builder()
    .withApiKey("YOUR_AZURE_OPENAI_API_KEY")
    .withDeploymentName("YOUR_DEPLOYMENT_NAME")
    .withEndpoint("YOUR_AZURE_ENDPOINT")
    .withModelId("text-embedding-ada-002")
    .build();
```

# [OpenAI](#tab/java-OpenAI)

```java
import com.microsoft.semantickernel.aiservices.openai.textembeddings.OpenAITextEmbeddingGenerationService;

OpenAITextEmbeddingGenerationService textEmbeddingService = OpenAITextEmbeddingGenerationService.builder()
    .withApiKey("YOUR_OPENAI_API_KEY")
    .withModelId("text-embedding-ada-002")
    .build();
```

# [VoyageAI](#tab/java-VoyageAI)

```java
import com.microsoft.semantickernel.aiservices.voyageai.core.VoyageAIClient;
import com.microsoft.semantickernel.aiservices.voyageai.textembedding.VoyageAITextEmbeddingGenerationService;

// Create VoyageAI client
VoyageAIClient client = new VoyageAIClient(System.getenv("VOYAGE_API_KEY"));

// Create embedding service
VoyageAITextEmbeddingGenerationService textEmbeddingService =
    VoyageAITextEmbeddingGenerationService.builder()
        .withClient(client)
        .withModelId("voyage-3-large")  // Options: voyage-3-large, voyage-code-3, voyage-finance-2, voyage-law-2
        .build();
```

---

## Using text embedding generation services

All text embedding generation services implement the `TextEmbeddingGenerationService` interface which provides methods to generate embeddings.

### Generating a single embedding

```java
import com.microsoft.semantickernel.services.textembedding.Embedding;
import reactor.core.publisher.Mono;

Mono<Embedding> embeddingMono = textEmbeddingService.generateEmbeddingAsync("Your text here");
Embedding embedding = embeddingMono.block();

System.out.println("Embedding dimension: " + embedding.getVector().size());
```

### Generating multiple embeddings

```java
import com.microsoft.semantickernel.services.textembedding.Embedding;
import java.util.Arrays;
import java.util.List;

List<String> texts = Arrays.asList(
    "First text",
    "Second text",
    "Third text"
);

List<Embedding> embeddings = textEmbeddingService.generateEmbeddingsAsync(texts).block();

for (int i = 0; i < embeddings.size(); i++) {
    System.out.println("Embedding " + i + " dimension: " + embeddings.get(i).getVector().size());
}
```

## VoyageAI Specialized Services

VoyageAI provides additional specialized embedding services for advanced use cases.

### Contextualized Embeddings

For RAG applications where you need document-aware chunk embeddings:

```java
import com.microsoft.semantickernel.aiservices.voyageai.contextualizedembedding.VoyageAIContextualizedEmbeddingGenerationService;

VoyageAIClient client = new VoyageAIClient(System.getenv("VOYAGE_API_KEY"));

VoyageAIContextualizedEmbeddingGenerationService service =
    VoyageAIContextualizedEmbeddingGenerationService.builder()
        .withClient(client)
        .withModelId("voyage-context-3")
        .build();

// Each inner list represents chunks from a document
List<List<String>> inputs = Arrays.asList(
    Arrays.asList("Chapter 1: Introduction", "This chapter covers...", "Key concepts include..."),
    Arrays.asList("Chapter 2: Methods", "We employed the following...")
);

List<Embedding> embeddings = service.generateContextualizedEmbeddingsAsync(inputs).block();
System.out.println("Generated " + embeddings.size() + " contextualized embeddings");
```

### Multimodal Embeddings

For text and image embeddings:

```java
import com.microsoft.semantickernel.aiservices.voyageai.multimodalembedding.VoyageAIMultimodalEmbeddingGenerationService;

VoyageAIClient client = new VoyageAIClient(System.getenv("VOYAGE_API_KEY"));

VoyageAIMultimodalEmbeddingGenerationService service =
    VoyageAIMultimodalEmbeddingGenerationService.builder()
        .withClient(client)
        .withModelId("voyage-multimodal-3")  // Options: voyage-multimodal-3, voyage-multimodal-3.5
        .build();

// Generate embeddings for text (also supports images)
List<String> texts = Arrays.asList("Text description 1", "Text description 2");
List<Embedding> embeddings = service.generateEmbeddingsAsync(texts).block();
System.out.println("Generated " + embeddings.size() + " multimodal embeddings");
```

::: zone-end
